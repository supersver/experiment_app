{
  "last_node_id": 16,
  "last_link_id": 15,
  "nodes": [
    {
      "id": 12,
      "type": "TextInput_",
      "pos": [
        162,
        -409
      ],
      "size": [
        516.7624029187252,
        415.1270026657445
      ],
      "flags": {},
      "order": 0,
      "mode": 0,
      "outputs": [
        {
          "name": "STRING",
          "type": "STRING",
          "links": [
            11
          ],
          "shape": 3,
          "slot_index": 0
        }
      ],
      "properties": {
        "Node name for S&R": "TextInput_"
      },
      "widgets_values": [
        "Format the raw transcript of a video, provided, into a structured document that will drive our content marketing pipeline. Focus on these key areas:\n\n1. Executive summary (3-5 bullet points)\n2. Sectioned content with descriptive subheadings (use markdown)\n3. Highlighted technical terms and product features (bold)\n4. Brief explanations of technical concepts [in brackets]\n5. Potential social media quotes {in braces}\n6. 'Dev Insight' tips as blockquotes after each section\n7. 'Further Resources' section suggesting related content\n8. 'Content Roadmap' outlining derivative content pieces\n\nAdditionally:\n\n- Mark areas needing expansion with [EXPAND]\n- Suggest natural points for call-to-action insertions with [CTA]\n\nPrioritize clarity and structure over comprehensiveness. Here's a mini-example of the expected format:\n\n### Introduction to DevGenius\n\n{QUOTE}DevGenius: Your AI-powered coding companion{QUOTE}\n\nDevGenius is an **AI-assisted code generation tool** designed to [streamline the development process by suggesting code snippets and refactoring existing code].\n\n[EXPAND: Include more details on AI model used]\n\n> Dev Insight: Always review and understand AI-generated code before integrating it into your project.\n> \n\n[CTA: Try DevGenius free for 30 days]\n\nPlease format a small section first and await feedback before proceeding with the entire transcript.\"\n\n\nFollowing is the transcript of the video:\n\n[1]"
      ]
    },
    {
      "id": 11,
      "type": "TextInput_",
      "pos": [
        249,
        -676
      ],
      "size": {
        "0": 400,
        "1": 200
      },
      "flags": {},
      "order": 1,
      "mode": 0,
      "outputs": [
        {
          "name": "STRING",
          "type": "STRING",
          "links": [
            10
          ],
          "shape": 3,
          "slot_index": 0
        }
      ],
      "properties": {
        "Node name for S&R": "TextInput_"
      },
      "widgets_values": [
        "0:01\nhello everyone this is Amon here from\n0:03\nflow scale and in this video I'm going\n0:07\nto be talking about uh IC light\n0:10\nsomething that has recently just come up\n0:12\nin uh uh and sort of is taking a lot of\n0:16\npeople by uh surprise because something\n0:19\nnow is possible which was not really\n0:22\npossible with\n0:24\num with a a stable diffusion which is to\n0:28\nbe able to adjust light in a fashion\n0:30\nthat is very\n0:32\nrealistic now uh we let's deep Let's uh\n0:35\ndig deep into that in this video but\n0:37\nbefore that uh uh if you want to reach\n0:40\nme out you can find me on Twitter with @\n0:42\nnerdycap\n0:44\n007 and uh I usually post about uh comfy\n0:48\nand uh uh how to work around comfy\n0:51\noptimizations on comfy and a lot of\n0:53\ninteresting stuff so let's get into\n0:57\nit so comfy you u i has this new even\n1:02\nbefore we lap into comfy UI let's talk\n1:06\nabout IC light\n1:09\nfirst so what is icite uh icite is a\n1:12\nproject to manipulate the illuminations\n1:14\nof image you can uh illuminate an image\n1:16\nin different fashion and uh not only\n1:19\nthat uh it is um it allows you to you\n1:23\nknow sort of add backgrounds and uh do\n1:25\nstuff with uh of by fixing a foreground\n1:28\nit it allows you to you know add\n1:30\nbackground and do a lot of stuff over\n1:31\nthere so to just give you an example\n1:35\nlike uh let's say this is an input image\n1:39\nuh you uh it automatically takes the\n1:41\nforeground out and uh uh uh based on the\n1:45\nprompt that you have provided and the\n1:46\ndirectional uh light that you have\n1:48\nprovided you can generate this sort of\n1:51\nimage now we'll experiment more with it\n1:55\num right now it has been launched on uh\n1:58\nboth on hugging phas and also on GitHub\n2:01\nthere are three models over here uh this\n2:05\none the FC IC light uncore sd15 uncore\n2:10\nfc. safe tensors is the one which is the\n2:13\nbest performing\n2:15\nmodel so we will uh use that across but\n2:19\nuh let's go through their um uh hugging\n2:22\nface space let's let's try this out\n2:25\nbefore we uh move on to comfy UI\n2:30\nso let me let me pick an image whom\n2:33\nshall we try let's try Jason mumua I\n2:36\nhave an image of Jason mumua\n2:38\nhere let's try Right\n2:41\nlight okay so they also have a few\n2:46\num uh options to select out of here\n2:49\nwhere which you can use as an example uh\n2:52\nbut let's let's see I I want to try\n2:54\nsomething on my own though\n2:58\num\n3:00\nman standing on a\n3:04\nbeach uh soft\n3:07\nsunlight I have put Right light let's\n3:10\nlet's try to relight this\n3:31\nvery\n3:33\ninteresting now if you take a look uh\n3:37\nthe light is so natural as if it's a\n3:40\nit's an image which was uh you know shot\n3:42\non a\n3:43\nbeach but uh there are a few like\n3:47\ndrawbacks to this process uh if you see\n3:50\nlike the distortions are happening a\n3:52\nlittle bit on the face and it's changing\n3:54\nthe details to some level although I I\n3:57\nmean real big kudos to\n4:00\nthe way they are uh still maintaining\n4:03\nthe details probably I guessing there's\n4:06\na\n4:07\ngood um sort of effort gone into just\n4:11\nretaining the details now this uh look\n4:14\nat this the hair and every everything is\n4:17\nso natural here the now right now these\n4:21\ndetails maybe are changing a little bit\n4:24\nbut I'm pretty much sure in the in the\n4:26\nuh in the uh later versions with of IC\n4:29\nlight we are going to be seeing like\n4:30\nreally amazing progress\n4:32\nthere so let's let's try a few few more\n4:36\nthings um and\n4:38\nstanding\n4:40\nin a living\n4:43\nroom um soft sunlight Through the\n4:48\nWindows coming through the\n4:51\nwindows let's let's put uh left light\n4:54\nthis time let's try\n5:05\no\n5:06\nnice but you can see uh the details of\n5:09\nthe clothing are changing like uh more\n5:13\nthan the details the coloring of the\n5:14\nclothing are\n5:15\nchanging so as as I already said like\n5:19\nit's\n5:20\nretaining things to a good level but\n5:23\nstill there is um there is a lot of\n5:27\nimprovement that needs to happen in\n5:29\nterms of uh retaining the identity to uh\n5:34\nits\n5:36\nfullest um they also like have a bunch\n5:39\nof examples you can try out here where\n5:42\nuh\n5:43\nthe they also have the seed and\n5:45\neverything here but uh so these I\n5:48\ngenerally when I try out uh um any new\n5:53\nuh you know uh space or any new model\n5:56\nthat gets launched I don't don't try\n5:58\nthere uh generic test data because it's\n6:02\nit's their best performing data you\n6:04\nalready know that right so it's better\n6:06\nto try out on your own uh inputs and uh\n6:09\nsee the uh how how exactly is the model\n6:13\nperforming so uh I think that's pretty\n6:16\nmuch it there are a few more options\n6:19\nAdvanced options here you can move\n6:21\naround the steps more steps let's maybe\n6:23\ntry\n6:24\nwith um 30\n6:30\n35 maybe let's let's try with\n6:34\n35 I don't usually go above 30 because\n6:37\nuh 30 is like enough uh for the number\n6:42\nof steps usually for me but let's\n6:45\nsee I'm just trying to see if uh we can\n6:49\nretain the these details which I'm\n6:51\ntalking about if we increase the steps\n6:53\nmaybe it retains it more who knows but\n6:57\nsee working with stable diffusion or any\n6:59\nuh thing like that is like uh shooting\n7:02\nan arrow in the dark but as you can see\n7:05\nlike it's again not retaining the\n7:08\nuh um the clothing but clothing not\n7:12\nbeing retained can be fixed a little bit\n7:14\nto a good certain uh level so if I um if\n7:18\nI go here and let me let me put pull\n7:22\nthis image out man standing in a living\n7:25\nroom um okay let's let's go through the\n7:28\nsame one actually let me relight this\n7:38\ndude oh my God\n7:41\nokay this is another uh issue here as\n7:45\nwell like it's darkening the uh the\n7:49\nperson the foreground a lot I don't know\n7:51\nhow to fix that\n7:53\nhere okay but but uh now if you see the\n7:57\nimage the the the the t-shirt is getting\n7:59\na little whitish over here let's let's\n8:01\ntry adding\n8:08\nthat actually yellow shirt um and black\n8:13\npants let's try adding\n8:26\nthat there you go uh so as you see right\n8:30\nuh you can do a lot of stuff like you\n8:32\ncan do uh um uh if there's a major use\n8:37\ncase of this in\n8:39\nfashion uh industry uh I mean the output\n8:43\nright now which you're seeing I'm pretty\n8:45\nmuch sure they are using an SD 1.5 model\n8:48\nat this point I guess I can just tell\n8:50\nfrom seeing the outputs but uh and also\n8:53\nwe are also using SD 1.5 in the comi\n8:56\nworkflows that I'm going to be showing\n8:58\nyou right after this but uh the the\n9:03\noutput uh here is good it is retaining\n9:06\nthe details the the texture and the\n9:09\nstructure of uh the clothing and the\n9:12\nperson uh as if it would happen like if\n9:15\nyou are using canny Edge detector to a\n9:18\nvery good level this something like this\n9:20\nyou would\n9:21\nsee but uh it's still changing the\n9:24\ndetails a little bit I'm pretty much\n9:26\nsure that will also improve as I already\n9:28\nsaid but uh\n9:30\nyou see right to a good level you can\n9:32\nsolve these issues by specifying the\n9:35\ncolor of the shirt pant and you know\n9:37\nspecific\n9:38\ndetails but um yeah there's a there's a\n9:43\nchance you will hit a roadblock uh when\n9:46\nwhen it's like any any sort of complex\n9:49\nclothing uh that's where you will hit a\n9:51\nroadblock\n9:52\nhere okay so let's see how we can use\n9:57\nthis in our comfy workflows what can we\n10:00\ndo here so to move on to the comfy side\n10:04\nI want to show you the\n10:07\ncomfy node that we're going to be using\n10:10\nit's comfy UI IC\n10:12\nlight and it is like uh pretty new 3 4\n10:17\ndays ago or probably last week people\n10:20\nhave been working on this very very new\n10:23\nso and they also specified some of the\n10:26\nexample workflows require the very\n10:28\nlatest featur features in KJ nodes so if\n10:31\nif you try out their example workflows\n10:34\nprobably just have KJ nodes I also have\n10:36\ninstalled KJ nodes it is mostly used for\n10:40\nthis uh lighting setup I'll show you\n10:42\nthat\n10:44\num yeah so let's let's uh dig deep into\n10:47\nthis I've already set up a a workflow\n10:50\nwhich I'll attach in the description of\n10:52\nthis video you can go through that and\n10:55\nlet's uh let's try to see what what we\n10:57\ncan achieve here\n11:00\nso uh just to explain the workflow uh\n11:03\nthis is the input image which I am\n11:05\ntaking in and uh first of all I'm using\n11:09\nuh image remg uh for removing the\n11:12\nbackground and just isolating the\n11:14\nforeground and uh after that that image\n11:18\nis being resized while also keeping\n11:21\nproportions which is very important if\n11:24\nyou lose the proportion it'll be well\n11:26\nyou'll lose the proportion so\n11:30\num uh after that this aspect resize mask\n11:35\nis coming from the mask area of this\n11:38\nI'll show you how exactly am I using\n11:41\nthis and then uh the output from this\n11:43\nthe mask that is being created is going\n11:44\ninto grow mask with blur these two nodes\n11:47\nare from KJ nodes and then we are\n11:49\nconverting it uh converting The Mask to\n11:52\nimage just to show case\n11:56\nhere and uh uh to Showcase here and also\n12:00\nwe are putting that in vaen code here\n12:03\nwhich is eventually going into a k\n12:05\nsampler\n12:08\nnow apart from that uh we are also using\n12:13\nas a base model we are using epic\n12:15\nrealism SD 1.5 model which is this\n12:19\nmodel the uh one uh which has been there\n12:24\nfor a while now\n12:27\nand that I'm connecting\n12:29\nto the load and apply IC light uh node\n12:35\nin which we are loading uh the sd15 FC\n12:39\nyou can download this uh uh uh this and\n12:42\nalso there is IC light conditioning you\n12:44\ncan download these models in uh um\n12:46\ncustom nodes right out of comfy so if\n12:49\nyou open comfy and uh let me check out\n12:52\nsee IC light uh if you just search IC\n12:55\nhyphen light you can find out this node\n12:57\ncomi IC light and you can directly\n12:59\ninstall this and if you want to install\n13:03\nthe models you can go and search I see\n13:07\nlight uh and you'll find all the three\n13:10\nmodels all three models have different\n13:12\npurposes but uh I generally I'm using uh\n13:16\nthe one which has FC because uh it's\n13:20\nwritten on their GitHub page that that's\n13:22\nthe one which is the most uh best\n13:24\nperforming uh model out of their uh uh\n13:27\ndifferent uh options\n13:32\nso uh the multiplier uh I'm not very\n13:35\nsure what exactly does it do but uh what\n13:39\nI found is like around. 2 183 or\n13:42\nwhatever at that uh uh number it's\n13:45\nworking decently\n13:48\nfine um there are some negative uh\n13:52\nnegative embeddings and negative uh um\n13:55\nuh prompts that I've given here and also\n13:57\nsome positive prompts which I have\n13:59\nconnected here uh\n14:02\nhere and\n14:04\neventually the K sampler throws an\n14:06\noutput VA decodes VA decode happens and\n14:10\nyou see the output here okay now let's\n14:12\ntry uh running with this also one more\n14:16\nuh thing which you won't find in the uh\n14:20\nin the uh hugging face space demo is\n14:23\nthat uh I have also introduced a IP\n14:26\nadapter here so that is a a good thing\n14:29\nuh that we can do like that's that's the\n14:32\ngood thing about comfi you have a lot of\n14:34\nfreedom in comi you can do a lot of\n14:35\nstuff so right now what I'm doing here\n14:37\nis I have introduced\n14:39\na uh IP adapter for uh giving a\n14:43\nreference image for the background right\n14:46\nso uh I'm doing something like that now\n14:48\nthis is not the traditional way to uh do\n14:51\nthe background based uh relighting on uh\n14:55\nIC light there is a separate way to uh\n14:59\nproperly uh you know uh overlay\n15:02\nsomething on a background but the\n15:04\nproblem in this is it just you know puts\n15:06\nthe person the foreground on this back\n15:09\nit takes this as a background and this\n15:11\ntakes this as a foreground and just puts\n15:13\nit on top of this right so I mean that\n15:18\nis doable but eventually it will look\n15:20\nquite fake to be very honest so uh but\n15:24\nbut the good thing is this is if you if\n15:27\nyou really look at it this is like in\n15:29\npainting on\n15:30\nsteroids because in painting so far has\n15:33\nbeen only\n15:35\num uh this is not particularly in\n15:38\npainting this is more like cut uh\n15:40\ncutting and pasting by mask or so in the\n15:43\nprevious previous uh you know uh uh um\n15:48\niterations if you if you did something\n15:50\nyou would be doing cutting and pasting\n15:52\nby mask but right now um you can do it\n15:56\nwith the lighting and everything in\n15:57\nplace this is so awesome otherwise what\n16:00\nusually used to happen is you cut by\n16:01\nmask and paste this dude here and then\n16:04\nyou run a k sampler on top of the final\n16:06\noutput uh final image to with a very\n16:09\nless denoising strength to make it as if\n16:11\nyou know it's blending in now what makes\n16:15\nthings blend in only lighting right so\n16:18\nthat got fixed so you don't have to run\n16:20\na separate uh sampling step right so\n16:24\nthere is a separate way to do this uh\n16:26\nbackground conditioning and everything\n16:29\nbut the way we are doing is quite\n16:30\ndifferent we are just introducing uh IP\n16:32\nadapter um because I just want to give a\n16:35\nreference image I don't want to uh do a\n16:37\nproper background uh sort of\n16:41\nsetting okay now let's run this uh\n16:43\nenough talking let's run this let's see\n16:47\nhow we can uh add some lighting on this\n16:51\nperson so let's say I want uh right side\n16:55\nleft side lighting okay let me put\n17:01\nthis\n17:04\nand\n17:06\nyep and I'll save\n17:13\nthis\n17:15\nokay uh photo of a man in a living room\n17:19\nlet me add\n17:20\n[Music]\n17:22\num see uh without yellow there there was\n17:25\nno yellow shirt so you see right it was\n17:27\nchanging the uh the\n17:30\ncolor yellow\n17:32\nshirt black\n17:35\npants black\n17:38\npants let's\n17:44\nsee so you can see the mask is\n17:48\ngenerated through the KJ nodes uh this\n17:52\nuh this node grow mask by with blur\n17:55\nresize mask through this\n18:01\nand\n18:02\nvoila there you\n18:06\ngo so uh now you can see right this uh\n18:10\nbackground image is coming from sort of\n18:14\nthis let you can also try with you know\n18:17\nwithout uh this of course let me try to\n18:20\nput this person in a let me let me\n18:22\nbypass this node for now let me try to\n18:25\nput this person in a garden or something\n18:28\nfor man in a\n18:34\ngarden\n18:55\nyeah yep beautiful\n18:59\nit's looking very good although the\n19:02\ncolor again is changing but it's it's\n19:05\ncoming out really well actually uh if I\n19:08\nwere to just look at it from uh a beauty\n19:12\nperspective it's coming out really\n19:17\nwell okay so that is IC light for you\n19:21\nguys and uh you can also uh I'll share\n19:26\nthis workflow in the chat in the\n19:29\nin the description of this video you can\n19:31\ngo ahead run this workflow on your end\n19:34\nthe best way to run any workflow is to\n19:35\njust load that workflow use manager to\n19:39\ninstall uh comfy manager to install your\n19:42\nuh Missing nodes and\n19:44\nmodels and you know just experiment\n19:47\nfurther so thanks again guys uh do not\n19:50\nforget to subscribe to flow scale uh we\n19:54\nwill we are posting uh different uh you\n19:57\nknow new innovations that are happening\n20:00\nin this space and how it can benefit\n20:03\ndifferent people uh so just tune in and\n20:06\nuh more interesting stuff coming uh\n20:08\nalong the lines thanks guys"
      ]
    },
    {
      "id": 10,
      "type": "PromptUtilitiesFormatString",
      "pos": [
        950,
        -608
      ],
      "size": {
        "0": 210,
        "1": 74
      },
      "flags": {},
      "order": 2,
      "mode": 0,
      "inputs": [
        {
          "name": "arg1",
          "type": "STRING",
          "link": 10,
          "widget": {
            "name": "arg1"
          },
          "slot_index": 0
        },
        {
          "name": "prompt",
          "type": "STRING",
          "link": 11,
          "widget": {
            "name": "prompt"
          }
        },
        {
          "name": "arg2",
          "type": "STRING",
          "link": null
        }
      ],
      "outputs": [
        {
          "name": "STRING",
          "type": "STRING",
          "links": [
            14
          ],
          "shape": 3,
          "slot_index": 0
        }
      ],
      "properties": {
        "Node name for S&R": "PromptUtilitiesFormatString"
      },
      "widgets_values": [
        "[1], [2]",
        ""
      ]
    },
    {
      "id": 14,
      "type": "ShowTextForGPT",
      "pos": [
        1444,
        -861
      ],
      "size": [
        635.5077474379309,
        622.5103667157625
      ],
      "flags": {},
      "order": 4,
      "mode": 0,
      "inputs": [
        {
          "name": "text",
          "type": "STRING",
          "link": 15,
          "widget": {
            "name": "text"
          }
        },
        {
          "name": "output_dir",
          "type": "STRING",
          "link": null,
          "widget": {
            "name": "output_dir"
          }
        }
      ],
      "outputs": [
        {
          "name": "STRING",
          "type": "STRING",
          "links": null,
          "shape": 6
        }
      ],
      "properties": {
        "Node name for S&R": "ShowTextForGPT"
      },
      "widgets_values": [
        "",
        "",
        "### Executive Summary\n- **IC Light:** A project enabling realistic light adjustments in images, including adding backgrounds and manipulating illuminations.\n- **Comfy UI Integration:** Recent integration with Comfy UI, showcasing workflow examples and the use of the Epic Realism SD 1.5 model.\n- **Technical Features:** Utilization of IC Light Conditioning, SD15 FC model, IP Adapter for reference images, and KJ nodes for enhanced lighting setups.\n- **Future Improvements:** Promising progress in retaining clothing details and enhancing output quality.\n\n### Introduction to IC Light\n[Include more details on IC Light and its applications]\n\n> Dev Insight: Experiment with different prompts and directional lights to achieve desired effects effectively.\n\n### Comfy UI Integration\n[Explore the workflow setup in Comfy UI with detailed steps and nodes used]\n\n> Dev Insight: Customize IP Adapter settings to experiment with various background settings for personalized imaging.\n\n### Further Resources\n- **Hugging Face Models:** Explore different models available on Hugging Face's platform for varied use cases.\n- **Comfy Workflow Examples:** Access the complete workflow and additional models for enhanced image manipulation.\n\n### Content Roadmap\n1. **Tutorial Video:** Visual demonstration on setting up an IC Light project in Comfy UI. [CTA: Watch the video for step-by-step guidance]\n2. **Detailed Blog Post:** Dive deeper into the technical aspects of IC Light and its integration with Comfy UI. [CTA: Read the blog for comprehensive insights]\n3. **Webinar on Image Processing:** Join our upcoming webinar discussing cutting-edge techniques in image processing using tools like IC Light and Comfy UI. [CTA: Register for the webinar]\n\nPlease expand on these points and refine for final content production."
      ]
    },
    {
      "id": 15,
      "type": "ChatGPTOpenAI",
      "pos": [
        871,
        -412
      ],
      "size": [
        400,
        358
      ],
      "flags": {},
      "order": 3,
      "mode": 0,
      "inputs": [
        {
          "name": "prompt",
          "type": "STRING",
          "link": 14,
          "widget": {
            "name": "prompt"
          }
        }
      ],
      "outputs": [
        {
          "name": "text",
          "type": "STRING",
          "links": [
            15
          ],
          "shape": 3,
          "slot_index": 0
        },
        {
          "name": "messages",
          "type": "STRING",
          "links": null,
          "shape": 3
        },
        {
          "name": "session_history",
          "type": "STRING",
          "links": null,
          "shape": 3
        }
      ],
      "properties": {
        "Node name for S&R": "ChatGPTOpenAI"
      },
      "widgets_values": [
        null,
        null,
        "",
        "You are ChatGPT, a large language model trained by OpenAI. Answer as concisely as possible.",
        "gpt-3.5-turbo",
        686658710350590,
        "randomize",
        1,
        null
      ]
    }
  ],
  "links": [
    [
      10,
      11,
      0,
      10,
      0,
      "STRING"
    ],
    [
      11,
      12,
      0,
      10,
      1,
      "STRING"
    ],
    [
      14,
      10,
      0,
      15,
      0,
      "STRING"
    ],
    [
      15,
      15,
      0,
      14,
      0,
      "STRING"
    ]
  ],
  "groups": [],
  "config": {},
  "extra": {
    "ds": {
      "scale": 0.4090909090909103,
      "offset": {
        "0": 370.3622675963824,
        "1": 1049.323084179533
      }
    }
  },
  "version": 0.4
}